---
layout: page
title: "Dynamic Programming"
category: tut
date: 2018-12-15 10:04:57
---

### **Introduction**

In this tutorial we will cover Dynamic Programming based methods for solving MDPs. We will
use as test case a variant of the FrozenLake environment from OpenAI-Gym. This variant will
give us access to the dynamics of the environments, such that we can run Dynamic Programming.

The topics to be covered include :

* Policy Evaluation
* Policy Iteration
* Value Iteration


### **The Environment**

The environment we will work with is the **FrozenLake** environment 


As mentioned earlier, the environment we will work with is a slight variant of the FrozenLake
environment. We just added a small tweak into the environment to get access to its dynamics. This
small tweak consists of just this one-liner :

```python
    ## Constructor of the frozen lake environment
    def __init__(self, desc=None, map_name="4x4",is_slippery=True):
        
        ## Some initialization over here ...

        P = {s : {a : [] for a in range(nA)} for s in range(nS)}

        ## Some initialization over there ...

        # obtain one-step dynamics for dynamic programming setting
        self.P = P

        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)
```

As you see, we just grab the dynamics into an instance variable for later usage. This dynamics can be
thought as the following transition model :

<!-- @IMG: Frozen Lake dynamics -->