{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning by Dynamic Programming\n",
    "\n",
    "In this tutorial we will use dynamic programming to solve a simple MDP task, represented by a simple **Grid world** environment. The outline is the following :\n",
    "\n",
    "* RL refresher\n",
    "* DP methods\n",
    "* Setting up the environment\n",
    "* Solution methods :\n",
    "    * Policy Evaluation\n",
    "    * Policy Iteration\n",
    "    * Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL refresher\n",
    "\n",
    "### RL formulation\n",
    "Recall the interaction loop of an agent with an environment :\n",
    "\n",
    "![rl loop](imgs/img_rl_interaction_loop.png)\n",
    "\n",
    "* The agent is in a certain state $s_{t} \\in \\lbrace S \\rbrace$ and it interacts with the environment via its actions $a_{t} \\in \\lbrace A \\rbrace$. \n",
    "\n",
    "* Because of this interaction, the environment returns back some information about how well this information went, namely the reward signal $r_{t+1}$ (a random variable from a distribution $R$), and the next state the agent landed $s_{t+1}$, by means of the dynamics of the environment (which can be modeled as $P(s^{'} | s, a)$).\n",
    "\n",
    "* The dynamics model and the reward distribution are usually thought as a single distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "    T(s^{'},r,s,a) = P( s^{'}, r | s, a )\n",
    "\\end{equation}\n",
    "\n",
    "* Then the process repeats until we get to a terminal state (episodic tasks) or it continues indefinitely (continuing tasks).\n",
    "\n",
    "* And the agents objective is to learn from this interaction in order to get the largest sum of rewards possible, which is called the **Return**, $G_{t}$\n",
    "\n",
    "\\begin{equation}\n",
    "    G_{t} = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t + k + 1}\n",
    "\\end{equation}\n",
    "\n",
    "* Here we are discounting with a factor of $\\gamma$ to make the sum bounded, and mostly to make the math work. Also, as described earlier, $r$ is a random variable, so the return is also a random variable. Because of this, we objective of the agent is to maximize the **Expected Return** :\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{E} \\lbrace G_{t} | s_t \\rbrace\n",
    "\\end{equation}\n",
    "\n",
    "So, we can formulate the RL problem mathematically by using this components $(S,A,T,\\gamma)$ into what is called a **Markov Decision Process (MDP)**, which is basically defined by that 4-tuple mentioned before, working as described earlier :\n",
    "\n",
    "* The agent in state $s$ from $S$ picks an action $a$ from $A$.\n",
    "* The environment takes one step using the dynamics $T$ and returns a reward $r$ and a new state $s^{'}$.\n",
    "* The process then repeats.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "* The dynamics of the environment only need the current state to compute the future state. This is called the **Markov Property**, and the state (environment state) satisfies it.\n",
    "* Usually the environment state is not visible to the agent. Instead just some observations $o_{t} \\in {O}$ are visible in return (which are a sort of state/configuration). If this observations are not sufficient to compute the future states, then the environment is said to be **Partially observable**, and we are in the case of a POMDP (Partially Observable Markov Decision Process)\n",
    "* Usually, to avoid this, the state representation given to the agent is made such that it can have enough information to satisfy this property (state augmentation), and usually this works well in Deep RL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL solution\n",
    "\n",
    "As described earlier,the objective of the agent is to get the largest expected return, and the way it can do this is by means of the actions it can take. So the agent has to pick actions accordingly. The agent has to pick an action in every environment step, so we can formulate this decision as a mapping : for a given state/configuration $s_{t}$ the agent is currently in, the agent chooses an action $a_{t}$. This mapping is called a **policy**, and can be :\n",
    "\n",
    "* Deterministic function mapping (Deterministic policy $\\pi(s)$) :\n",
    "    \\begin{equation}\n",
    "        a_{t} = \\pi(s_{t})\n",
    "    \\end{equation}\n",
    "    \n",
    "* Sampling from a distribution (Stochastic policy $\\pi(a|s)$) :\n",
    "    \\begin{equation}\n",
    "        a_{t} \\sim \\pi(a | s_{t})\n",
    "    \\end{equation}\n",
    "    \n",
    "So, the objective of the agent is to find a **policy** $\\pi$ (deterministic or stochastic) such that if the agent follows this policy it will **maximize the expected return** it gets from its interaction with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure paths\n",
    "import sys\n",
    "sys.path.insert( 0, '../' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
